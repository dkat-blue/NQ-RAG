{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline for Natural Questions Dataset (Modular)\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) pipeline to answer questions from the Natural Questions (NQ) dataset. This version is modularized, with core logic in the `src` directory.\n",
    "\n",
    "**Pipeline Components (from `src.config`):**\n",
    "- **Dataset:** Natural Questions (Simplified Version)\n",
    "- **Orchestration:** LlamaIndex\n",
    "- **Vector Store:** Qdrant\n",
    "- **Embedding Model & LLMs:** Configured via `src.config` and `src.llm_setup`\n",
    "\n",
    "**Evaluation:**\n",
    "- **Retrieval:** Recall@K\n",
    "- **Generation (Reference-based):** ROUGE, BLEU\n",
    "- **Generation (Reference-free & Reference-aware):** LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f5cb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU llama-index llama-index-vector-stores-qdrant llama-index-embeddings-huggingface llama-index-llms-openai qdrant-client sentence-transformers nltk rouge_score datasets tqdm pandas scikit-learn ipywidgets notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe16822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Set: LM_STUDIO_API_BASE is now: http://192.168.0.114:1234/v1\n",
      "Notebook Set: OPENAI_API_KEY is now: not-needed\n",
      "Notebook Set: OPENAI_API_BASE is now: http://192.168.0.114:1234/v1\n"
     ]
    }
   ],
   "source": [
    "# Cell [2]\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "# Set LM Studio API base if it's not already an environment variable elsewhere\n",
    "# Ensure this IP and port are correct for the LM Studio instance\n",
    "os.environ[\"LM_STUDIO_API_BASE\"] = \"http://192.168.0.114:1234/v1\"\n",
    "# For OpenAI compatible endpoints like LM Studio, the API key is often not needed or can be any string\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"not-needed\" # Or your actual key if required by a different service\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"LM_STUDIO_API_BASE\"] # Align OpenAI base with LM Studio for LlamaIndex\n",
    "\n",
    "print(f\"Notebook Set: LM_STUDIO_API_BASE is now: {os.getenv('LM_STUDIO_API_BASE')}\")\n",
    "print(f\"Notebook Set: OPENAI_API_KEY is now: {os.getenv('OPENAI_API_KEY')}\")\n",
    "print(f\"Notebook Set: OPENAI_API_BASE is now: {os.getenv('OPENAI_API_BASE')}\")\n",
    "\n",
    "\n",
    "import llama_index.llms.openai.utils as oi_utils\n",
    "# Add custom model names and their context window sizes to LlamaIndex's registry\n",
    "# The value should be the actual context window set in LM Studio (e.g., 8192).\n",
    "# src.config.MODEL_CTX_WINDOW will be the source of truth for budgeting.\n",
    "# These oi_utils are more for LlamaIndex's internal model DB if it uses it for OpenAI class.\n",
    "oi_utils.ALL_AVAILABLE_MODELS[\"gemma-3-4b-it\"] = 8192 # Match cfg.MODEL_CTX_WINDOW\n",
    "oi_utils.ALL_AVAILABLE_MODELS[\"gemma-3-27b-it\"] = 8192 # Match cfg.MODEL_CTX_WINDOW (or specific judge window if different)\n",
    "\n",
    "# Mark them as chat models\n",
    "oi_utils.CHAT_MODELS.update({\"gemma-3-4b-it\": True, \"gemma-3-27b-it\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc3a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, JSON # For pretty printing JSON and DataFrames\n",
    "\n",
    "# Add project root to sys.path to allow imports from src\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import from custom modules\n",
    "from src import config as cfg \n",
    "from src.data_loader import load_nq_data\n",
    "from src.text_processing import extract_and_filter_text_from_simplified_doc \n",
    "from src.data_preparation import prepare_kb_and_test_set\n",
    "from src.llm_setup import setup_qdrant_client, setup_embedding_model, setup_llm, configure_llama_index_settings\n",
    "from src.rag_pipeline_logic import index_knowledge_base, generate_no_rag_answers, generate_rag_answers\n",
    "from src.evaluation_utils import evaluate_recall_at_k, calculate_rouge_l_and_bleu, evaluate_with_llm_judge\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core import Settings # For accessing global settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c5c88",
   "metadata": {},
   "source": [
    "## 2. Configure Logging and Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e4211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:03:27,956 - __main__ - INFO - Notebook execution started. Random seed: 42\n",
      "2025-06-01 15:03:27,957 - __main__ - INFO - Data will be loaded from: /home/denis/kpi/iasa_nlp_labs/nq_rag/data/v1.0-simplified-nq-train.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(stream=sys.stdout, \n",
    "                    level=getattr(logging, cfg.LOG_LEVEL.upper(), logging.INFO),\n",
    "                    format=cfg.LOG_FORMAT)\n",
    "logger = logging.getLogger(__name__) # For notebook specific logs\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "random.seed(cfg.RANDOM_SEED)\n",
    "np.random.seed(cfg.RANDOM_SEED)\n",
    "\n",
    "logger.info(f\"Notebook execution started. Random seed: {cfg.RANDOM_SEED}\")\n",
    "logger.info(f\"Data will be loaded from: {cfg.NQ_TRAIN_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1779686",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "Performing a brief EDA on a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6afaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:03:27,996 - __main__ - INFO - Starting EDA: Loading up to 1000 examples for inspection...\n",
      "2025-06-01 15:03:27,998 - src.data_loader - INFO - Attempting to load NQ data from: /home/denis/kpi/iasa_nlp_labs/nq_rag/data/v1.0-simplified-nq-train.jsonl.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4cfc41521b418d9ae166fa0161a920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading NQ data: 0 lines [00:00, ? lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:03:28,271 - src.data_loader - INFO - Reached max_examples limit of 1000. Loaded 1000 examples.\n",
      "2025-06-01 15:03:28,272 - src.data_loader - INFO - Successfully loaded 1000 examples from /home/denis/kpi/iasa_nlp_labs/nq_rag/data/v1.0-simplified-nq-train.jsonl.gz\n",
      "2025-06-01 15:03:28,273 - __main__ - INFO - EDA: Loaded 1000 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3c4d2d329447e6ad0b086f1538b91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Performing EDA on NQ data:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NQ Dataset EDA Results (Sample) ---\n",
      "total_examples_inspected: 1000\n",
      "with_document_text_field: 1000\n",
      "with_long_answer_candidates_list: 1000\n",
      "with_non_empty_long_answer_candidates: 1000\n",
      "examples_with_at_least_one_valid_candidate_text: 1000\n",
      "total_candidates_inspected: 139479\n",
      "valid_candidate_tokens_spans: 139479\n",
      "candidates_with_non_empty_extracted_text: 138640\n",
      "with_example_id: 1000\n",
      "with_question_text: 1000\n",
      "\n",
      "% with 'document_text': 100.00%\n",
      "% with non-empty 'long_answer_candidates': 100.00%\n",
      "% producing at least one KB doc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Starting EDA: Loading up to {cfg.EDA_SAMPLE_SIZE} examples for inspection...\")\n",
    "eda_data = load_nq_data(cfg.NQ_TRAIN_PATH, max_examples=cfg.EDA_SAMPLE_SIZE)\n",
    "\n",
    "if not eda_data:\n",
    "    logger.error(\"EDA: No data loaded. Cannot perform EDA.\")\n",
    "else:\n",
    "    logger.info(f\"EDA: Loaded {len(eda_data)} examples.\")\n",
    "    stats = {\n",
    "        \"total_examples_inspected\": len(eda_data),\n",
    "        \"with_document_text_field\": 0,\n",
    "        \"with_long_answer_candidates_list\": 0,\n",
    "        \"with_non_empty_long_answer_candidates\": 0,\n",
    "        \"examples_with_at_least_one_valid_candidate_text\": 0,\n",
    "        \"total_candidates_inspected\": 0,\n",
    "        \"valid_candidate_tokens_spans\": 0,\n",
    "        \"candidates_with_non_empty_extracted_text\": 0,\n",
    "        \"with_example_id\": 0,\n",
    "        \"with_question_text\": 0\n",
    "    }\n",
    "\n",
    "    from tqdm.notebook import tqdm \n",
    "    for i, example in enumerate(tqdm(eda_data, desc=\"Performing EDA on NQ data\")):\n",
    "        if example.get('example_id'): stats[\"with_example_id\"] += 1\n",
    "        if example.get('question_text'): stats[\"with_question_text\"] += 1\n",
    "        \n",
    "        has_doc_text = False\n",
    "        doc_text_content = example.get('document_text')\n",
    "        if doc_text_content and isinstance(doc_text_content, str):\n",
    "            stats[\"with_document_text_field\"] += 1\n",
    "            has_doc_text = True\n",
    "            doc_tokens_list = doc_text_content.split(' ') \n",
    "\n",
    "        if 'long_answer_candidates' in example: stats[\"with_long_answer_candidates_list\"] += 1\n",
    "        candidates = example.get('long_answer_candidates', [])\n",
    "        if candidates: stats[\"with_non_empty_long_answer_candidates\"] += 1\n",
    "        \n",
    "        example_produced_kb_doc = False\n",
    "        if has_doc_text and candidates:\n",
    "            for cand in candidates:\n",
    "                stats[\"total_candidates_inspected\"] += 1\n",
    "                if not isinstance(cand, dict): continue\n",
    "                start_token, end_token = cand.get('start_token', -1), cand.get('end_token', -1)\n",
    "                if start_token != -1 and end_token != -1 and start_token < end_token:\n",
    "                    stats[\"valid_candidate_tokens_spans\"] += 1\n",
    "                    # Pass the pre-split doc_tokens_list for EDA consistency\n",
    "                    text_content = extract_and_filter_text_from_simplified_doc(doc_tokens_list, start_token, end_token)\n",
    "                    if text_content:\n",
    "                        stats[\"candidates_with_non_empty_extracted_text\"] += 1\n",
    "                        example_produced_kb_doc = True\n",
    "        if example_produced_kb_doc: stats[\"examples_with_at_least_one_valid_candidate_text\"] += 1\n",
    "\n",
    "    print(\"\\n--- NQ Dataset EDA Results (Sample) ---\")\n",
    "    for key, value in stats.items(): print(f\"{key}: {value}\")\n",
    "    if stats[\"total_examples_inspected\"] > 0:\n",
    "        print(f\"\\n% with 'document_text': { (stats['with_document_text_field'] / stats['total_examples_inspected']) * 100 :.2f}%\")\n",
    "        print(f\"% with non-empty 'long_answer_candidates': { (stats['with_non_empty_long_answer_candidates'] / stats['total_examples_inspected']) * 100 :.2f}%\")\n",
    "        print(f\"% producing at least one KB doc: { (stats['examples_with_at_least_one_valid_candidate_text'] / stats['total_examples_inspected']) * 100 :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488b90d",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preparation\n",
    "Load the full dataset (or `KNOWLEDGE_BASE_SIZE` limit) and prepare the knowledge base documents and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38d81c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:03:29,429 - __main__ - INFO - Loading NQ data for KB and Test Set (up to 20000 examples)...\n",
      "2025-06-01 15:03:29,430 - src.data_loader - INFO - Attempting to load NQ data from: /home/denis/kpi/iasa_nlp_labs/nq_rag/data/v1.0-simplified-nq-train.jsonl.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5375bfd61b94474fa322e0efc631b03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading NQ data: 0 lines [00:00, ? lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:03:34,872 - src.data_loader - INFO - Reached max_examples limit of 20000. Loaded 20000 examples.\n",
      "2025-06-01 15:03:34,873 - src.data_loader - INFO - Successfully loaded 20000 examples from /home/denis/kpi/iasa_nlp_labs/nq_rag/data/v1.0-simplified-nq-train.jsonl.gz\n",
      "2025-06-01 15:03:34,874 - __main__ - INFO - Preparing Knowledge Base and Test Set from 20000 loaded examples...\n",
      "2025-06-01 15:03:34,874 - src.data_preparation - INFO - Building Knowledge Base with optimized candidate selection...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99eeaf37dab4608ada92b9b1c0eed66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building KB:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:04:00,892 - src.data_preparation - INFO - Total candidates processed: 2621615, Skipped: 1832032\n",
      "2025-06-01 15:04:00,893 - src.data_preparation - INFO - Built KB with 789583 LlamaIndex documents (after optimized selection).\n",
      "2025-06-01 15:04:00,893 - src.data_preparation - INFO - Screening examples for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c86a32cdd8e4fe3909d249cb64cb950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Screening for test set:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:04:04,618 - src.data_preparation - INFO - Selected 300 examples for the test set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8435febe06384d8885e329f724fd43b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:04:04,689 - src.data_preparation - INFO - Prepared 300 test examples.\n",
      "2025-06-01 15:04:04,690 - src.data_preparation - INFO - Prepared gold map for recall for 300 questions.\n",
      "\n",
      "--- Data Preparation Summary ---\n",
      "Knowledge Base documents created: 789583\n",
      "Test examples prepared: 300\n",
      "Gold map for recall entries: 300\n",
      "\n",
      "Sample test set item:\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "example_id": "2765840165344359847",
       "question_text": "when does the movie shot caller come out in theaters",
       "target_answer": "August 18 , 2017"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample KB Llama Document (first 200 chars):\n",
      "( hide ) This article has multiple issues . Please help improve it or discuss these issues on the talk page . ( Learn how and when to remove these template messages ) This article needs additional cit...\n",
      "Metadata: {'example_id': '5655493461695504401', 'candidate_index': 0, 'is_gold': False, 'is_top_level': True, 'original_question': 'which is the most common use of opt-in e-mail marketing', 'document_url': 'https://en.wikipedia.org//w/index.php?title=Email_marketing&amp;oldid=814071202'}\n",
      "\n",
      "Sample gold_map_for_recall entry:\n",
      "Q: when does the movie shot caller come out in theaters -> Gold Doc ID: 2765840165344359847_20\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Loading NQ data for KB and Test Set (up to {cfg.KNOWLEDGE_BASE_SIZE} examples)...\")\n",
    "all_nq_data = load_nq_data(cfg.NQ_TRAIN_PATH, max_examples=cfg.KNOWLEDGE_BASE_SIZE)\n",
    "\n",
    "test_set = []\n",
    "kb_docs_llama = []\n",
    "gold_map_for_recall = {}\n",
    "\n",
    "if not all_nq_data:\n",
    "    logger.error(\"No data loaded. Cannot proceed with KB and Test Set preparation.\")\n",
    "else:\n",
    "    logger.info(f\"Preparing Knowledge Base and Test Set from {len(all_nq_data)} loaded examples...\")\n",
    "    test_set, kb_docs_llama, gold_map_for_recall = prepare_kb_and_test_set(\n",
    "        all_nq_data, \n",
    "        cfg.NUM_TEST_EXAMPLES,\n",
    "        random_seed=cfg.RANDOM_SEED\n",
    "    )\n",
    "\n",
    "print(\"\\n--- Data Preparation Summary ---\")\n",
    "print(f\"Knowledge Base documents created: {len(kb_docs_llama)}\")\n",
    "print(f\"Test examples prepared: {len(test_set)}\")\n",
    "print(f\"Gold map for recall entries: {len(gold_map_for_recall)}\")\n",
    "\n",
    "if test_set:\n",
    "    print(\"\\nSample test set item:\")\n",
    "    display(JSON(test_set[0]))\n",
    "if kb_docs_llama:\n",
    "    print(\"\\nSample KB Llama Document (first 200 chars):\")\n",
    "    # Ensure get_content is called correctly; metadata_mode=\"all\" might not be standard for Document\n",
    "    try:\n",
    "        print(kb_docs_llama[0].get_content()[:200] + \"...\") \n",
    "    except AttributeError:\n",
    "        print(kb_docs_llama[0].text[:200] + \"...\") # Fallback to .text attribute\n",
    "    print(f\"Metadata: {kb_docs_llama[0].metadata}\")\n",
    "if gold_map_for_recall and test_set:\n",
    "    try:\n",
    "        sample_q_text = test_set[0]['question_text']\n",
    "        if sample_q_text in gold_map_for_recall:\n",
    "            print(\"\\nSample gold_map_for_recall entry:\")\n",
    "            print(f\"Q: {sample_q_text} -> Gold Doc ID: {gold_map_for_recall[sample_q_text]}\")\n",
    "    except IndexError:\n",
    "        logger.warning(\"Could not display sample gold_map_for_recall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb7eb0",
   "metadata": {},
   "source": [
    "## 5. Setup Vector Store, Embedding Model, and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:04:04,739 - __main__ - INFO - Setting up Qdrant client, embedding model, and LLMs...\n",
      "2025-06-01 15:04:04,827 - src.llm_setup - INFO - Successfully connected to Qdrant at http://localhost:6333 and listed collections.\n",
      "2025-06-01 15:04:05,017 - src.llm_setup - INFO - Attempting to set up embedding model 'intfloat/e5-small-v2' on device: 'cuda' with embed_batch_size: 512\n",
      "2025-06-01 15:04:05,025 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: intfloat/e5-small-v2\n",
      "2025-06-01 15:04:08,017 - sentence_transformers.SentenceTransformer - INFO - 2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2025-06-01 15:04:08,233 - src.llm_setup - INFO - Successfully loaded embedding model: intfloat/e5-small-v2 on device 'cuda'\n",
      "2025-06-01 15:04:08,234 - src.llm_setup - INFO - Successfully initialized Generator LLM: gemma-3-4b-it via http://192.168.0.114:1234/v1 with temperature: 0.0\n",
      "2025-06-01 15:04:08,235 - src.llm_setup - INFO - Successfully initialized Judge LLM: gemma-3-27b-it via http://192.168.0.114:1234/v1 with temperature: 0.0\n",
      "2025-06-01 15:04:08,236 - src.llm_setup - WARNING - LLM instance does not have a 'tokenizer' attribute. Global Settings.tokenizer might not be optimally set. Using LlamaIndex default.\n",
      "2025-06-01 15:04:08,338 - src.llm_setup - INFO - LlamaIndex Settings configured (or attempted).\n",
      "2025-06-01 15:04:08,339 - __main__ - INFO - LlamaIndex global settings have been configured.\n",
      "2025-06-01 15:04:08,339 - __main__ - INFO - LlamaIndex global tokenizer configured: functools.partial(<bound method Encoding.encode of <Encoding 'cl100k_base'>>, allowed_special='all')\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Setting up Qdrant client, embedding model, and LLMs...\")\n",
    "\n",
    "qdrant_client_instance = setup_qdrant_client(cfg.QDRANT_URL)\n",
    "embed_model = setup_embedding_model(\n",
    "    cfg.EMBED_MODEL_NAME,\n",
    "    batch_size=cfg.EMBED_BATCH_SIZE\n",
    "    # device_preference=\"cuda\" is default in setup_embedding_model if available\n",
    ")\n",
    "\n",
    "# Setup LLMs with temperature from config or new default (0.0)\n",
    "# The setup_llm function now defaults to temperature=0.0\n",
    "generator_llm = setup_llm(\n",
    "    cfg.LLM_API_BASE,\n",
    "    cfg.LLM_API_KEY,\n",
    "    cfg.GENERATOR_LLM_MODEL_NAME,\n",
    "    \"Generator\"\n",
    ")\n",
    "judge_llm = setup_llm(\n",
    "    cfg.LLM_API_BASE,\n",
    "    cfg.LLM_API_KEY,\n",
    "    cfg.JUDGE_LLM_MODEL_NAME,\n",
    "    \"Judge\"\n",
    ")\n",
    "\n",
    "if generator_llm and embed_model:\n",
    "    # Pass the generator_llm which now has the tokenizer attribute set up by LlamaOpenAI\n",
    "    configure_llama_index_settings(generator_llm, embed_model, cfg.CHUNK_SIZE, cfg.CHUNK_OVERLAP)\n",
    "    logger.info(\"LlamaIndex global settings have been configured.\")\n",
    "    if Settings.tokenizer:\n",
    "        logger.info(f\"LlamaIndex global tokenizer configured: {Settings.tokenizer}\")\n",
    "    else:\n",
    "        logger.warning(\"LlamaIndex global tokenizer could not be configured from LLM.\")\n",
    "else:\n",
    "    logger.error(\"Critical components (Generator LLM or Embedding Model) failed to initialize. LlamaIndex settings not fully configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6e6f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: LM_STUDIO_API_BASE from os.getenv is: http://192.168.0.114:1234/v1\n",
      "DEBUG: cfg.LLM_API_BASE is: http://192.168.0.114:1234/v1\n"
     ]
    }
   ],
   "source": [
    "# Temporary Debug Cell\n",
    "import os\n",
    "print(f\"DEBUG: LM_STUDIO_API_BASE from os.getenv is: {os.getenv('LM_STUDIO_API_BASE')}\")\n",
    "print(f\"DEBUG: cfg.LLM_API_BASE is: {cfg.LLM_API_BASE}\") # Assuming cfg is already imported from src.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "656f42e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. \n",
      "\n",
      "Itâ€™s a global center for art, fashion, gastronomy and culture. ðŸ˜Š \n",
      "\n",
      "Do you want to know anything more about Paris or France in general?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "import src.config as cfg # Make sure cfg is imported if not already in this cell's scope\n",
    "test_llm = OpenAI(\n",
    "    base_url = os.getenv(\"LM_STUDIO_API_BASE\"), # Use env var\n",
    "    api_key  = os.getenv(\"OPENAI_API_KEY\"),    # Use env var\n",
    "    model    = cfg.GENERATOR_LLM_MODEL_NAME, # Use from config\n",
    ")\n",
    "print(test_llm.complete(\"What is the capital of France?\").text) # Changed \"ping\" to a more standard query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159a072",
   "metadata": {},
   "source": [
    "## 6. Generation - Baseline (Without RAG)\n",
    "\n",
    "To understand the impact of RAG, we first establish a baseline. This involves asking the generator LLM (e.g., `gemma-3-4b-it` from `config.py`) to answer questions from our test set *without* providing any retrieved context from our knowledge base.\n",
    "\n",
    "-   The LLM relies solely on its pre-trained knowledge.\n",
    "-   The responses are collected for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6206168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:04:09,429 - __main__ - INFO - Starting indexing/loading for Qdrant collection 'nq_rag_lab_collection'. Recreate: False\n",
      "2025-06-01 15:04:09,434 - src.rag_pipeline_logic - INFO - Qdrant collection 'nq_rag_lab_collection' already exists.\n",
      "2025-06-01 15:04:09,434 - src.rag_pipeline_logic - INFO - Attempting to load existing VectorStoreIndex from Qdrant collection 'nq_rag_lab_collection'.\n",
      "2025-06-01 15:04:09,440 - src.rag_pipeline_logic - WARNING - Loaded index from collection 'nq_rag_lab_collection', but it appears to be empty (vectors_count: None). You might need to recreate it by setting RECREATE_QDRANT_COLLECTION = True in config.\n",
      "2025-06-01 15:04:09,440 - __main__ - INFO - VectorStoreIndex obtained successfully.\n"
     ]
    }
   ],
   "source": [
    "vector_store = None\n",
    "vector_index = None\n",
    "\n",
    "# Ensure global LlamaIndex Settings are correctly populated before calling index_knowledge_base\n",
    "# This check is more robust here, before the main 'if' block for indexing.\n",
    "if not Settings.embed_model or not Settings.node_parser:\n",
    "    logger.error(\"LlamaIndex global Settings (embed_model or node_parser) not configured. Cannot proceed with indexing.\")\n",
    "    logger.error(f\"Settings.embed_model: {Settings.embed_model}\")\n",
    "    logger.error(f\"Settings.node_parser: {Settings.node_parser}\")\n",
    "elif qdrant_client_instance: # We need qdrant client. kb_docs_llama can be empty if we are loading an existing index.\n",
    "    logger.info(f\"Starting indexing/loading for Qdrant collection '{cfg.QDRANT_COLLECTION_NAME}'. Recreate: {cfg.RECREATE_QDRANT_COLLECTION}\")\n",
    "    \n",
    "    vector_store, vector_index = index_knowledge_base(\n",
    "        qdrant_client_instance=qdrant_client_instance,\n",
    "        collection_name=cfg.QDRANT_COLLECTION_NAME,\n",
    "        embedding_dim=cfg.EMBEDDING_DIM,\n",
    "        kb_docs_llama=kb_docs_llama,  # Pass the loaded documents for new indexing\n",
    "        # embed_model_instance and node_parser_instance are removed as they are now taken from global Settings\n",
    "        recreate_collection=cfg.RECREATE_QDRANT_COLLECTION # Pass the flag from config\n",
    "    )\n",
    "    \n",
    "    if vector_index:\n",
    "        logger.info(\"VectorStoreIndex obtained successfully.\")\n",
    "    else:\n",
    "        logger.error(\"Failed to obtain VectorStoreIndex.\")\n",
    "else:\n",
    "    missing_components = []\n",
    "    if not qdrant_client_instance: missing_components.append(\"Qdrant client\")\n",
    "    # embed_model check is implicitly covered by Settings.embed_model check above\n",
    "    logger.warning(f\"Skipping indexing due to missing components: {', '.join(missing_components)} or unconfigured LlamaIndex Settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9c5cd8",
   "metadata": {},
   "source": [
    "## 7. Generation - With RAG\n",
    "\n",
    "Now, we implement the full RAG pipeline for generation:\n",
    "\n",
    "1.  **Retrieve:** For each question in the test set, use the configured retriever to fetch the `TOP_K_RETRIEVAL_FOR_GENERATION` most relevant document chunks from Qdrant.\n",
    "2.  **Augment:** Construct a prompt for the generator LLM. This prompt includes:\n",
    "3.  **Generate:** The LLM generates an answer based on the augmented prompt. \n",
    "\n",
    "The `rag_pipeline_logic.py` contains the core function for this query-retrieve-generate process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3b04e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:04:09,480 - __main__ - INFO - Generating baseline answers (No-RAG)...\n",
      "2025-06-01 15:04:09,481 - src.rag_pipeline_logic - INFO - Generating answers without RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfa95643ccc42cfadfc2bd2d327657a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "No-RAG Generation:   0%|          | 0/300 [00:00<?, ? questions/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample No-RAG Answer ---\n",
      "Question: when does the movie shot caller come out in theaters\n",
      "Generated (No-RAG): â€œShot Callerâ€ was released in theaters on **August 25, 2023**. \n",
      "\n",
      "You can find more details and showtimes here: [https://www.imdb.com/title/tt14968736/](https://www.imdb.com/title/tt14968736/)\n",
      "Reference: August 18 , 2017\n"
     ]
    }
   ],
   "source": [
    "no_rag_answers = []\n",
    "if generator_llm and test_set:\n",
    "    logger.info(\"Generating baseline answers (No-RAG)...\")\n",
    "    no_rag_answers = generate_no_rag_answers(generator_llm, test_set)\n",
    "    if no_rag_answers and len(no_rag_answers) == len(test_set) and test_set:\n",
    "        print(\"\\n--- Sample No-RAG Answer ---\")\n",
    "        print(f\"Question: {test_set[0]['question_text']}\")\n",
    "        print(f\"Generated (No-RAG): {no_rag_answers[0]}\")\n",
    "        print(f\"Reference: {test_set[0]['target_answer']}\")\n",
    "    elif not test_set:\n",
    "        logger.info(\"Test set is empty, no sample to show.\")\n",
    "    else:\n",
    "        logger.warning(\"No-RAG answer generation might have failed or produced mismatched results.\")\n",
    "else:\n",
    "    logger.warning(\"Generator LLM not ready or no test set. Skipping No-RAG generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f3249",
   "metadata": {},
   "source": [
    "## 8. Evaluating Generation Performance\n",
    "\n",
    "This is where we quantitatively and qualitatively assess the answers produced by both the baseline (No-RAG) and RAG approaches.\n",
    "\n",
    "-   **Quantitative Metrics:**\n",
    "    * **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** Measures overlap between the generated answer and a reference answer (n-gram overlap, longest common subsequence).\n",
    "    * **BLEU (Bilingual Evaluation Understudy):** Another metric based on n-gram precision, often used in machine translation but also applied to QA.\n",
    "-   **Qualitative Metrics (LLM-as-a-Judge):**\n",
    "    * A more powerful LLM (e.g., `gemma-3-27b-it` as `JUDGE_LLM_MODEL_NAME` from `config.py`) is used to evaluate the generated answers based on criteria like:\n",
    "        * **Correctness/Relevance:** Is the answer accurate and relevant to the question?\n",
    "        * **Faithfulness (for RAG):** Is the answer well-grounded in the provided context? Does it avoid hallucinating information not present in the context?\n",
    "    * Prompts for the judge LLM are carefully designed to elicit these evaluations.\n",
    "-   The `evaluation_utils.py` script contains the logic for these evaluation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a4a4e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:11:27,389 - __main__ - INFO - Generating answers with RAG...\n",
      "2025-06-01 15:11:27,390 - src.rag_pipeline_logic - INFO - Setting up RAG retriever with top_k_retrieval = 5\n",
      "2025-06-01 15:11:27,391 - src.rag_pipeline_logic - INFO - Generating answers with RAG...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231e757cecd346f6b722a863c57f32f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RAG Generation:   0%|          | 0/300 [00:00<?, ? questions/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample RAG Answer ---\n",
      "Question: when does the movie shot caller come out in theaters\n",
      "Retrieved Context (RAG - first 300 chars): It premiered at the Los Angeles Film Festival on June 16 , 2017 . It was released on July 20 , 2017 , through DirecTV Cinema , and received a theatrical release on August 18 , 2017 by Saban Films .\\n\\nShot Caller is an American crime thriller film directed and written by Ric Roman Waugh . The film s...\n",
      "Generated (RAG): August 18, 2017\n",
      "Reference: August 18 , 2017\n"
     ]
    }
   ],
   "source": [
    "rag_answers = []\n",
    "retrieved_contexts_for_rag = []\n",
    "\n",
    "if vector_index and generator_llm and test_set:\n",
    "    logger.info(\"Generating answers with RAG...\")\n",
    "    rag_answers, retrieved_contexts_for_rag = generate_rag_answers(\n",
    "        vector_index, \n",
    "        generator_llm, \n",
    "        test_set, \n",
    "        cfg.TOP_K_RETRIEVAL_FOR_GENERATION\n",
    "    )\n",
    "    if rag_answers and retrieved_contexts_for_rag and len(rag_answers) == len(test_set) and test_set:\n",
    "        print(\"\\n--- Sample RAG Answer ---\")\n",
    "        print(f\"Question: {test_set[0]['question_text']}\")\n",
    "        print(f\"Retrieved Context (RAG - first 300 chars): {str(retrieved_contexts_for_rag[0])[:300]}...\")\n",
    "        print(f\"Generated (RAG): {rag_answers[0]}\")\n",
    "        print(f\"Reference: {test_set[0]['target_answer']}\")\n",
    "    elif not test_set:\n",
    "        logger.info(\"Test set is empty, no sample to show.\")\n",
    "    else:\n",
    "        logger.warning(\"RAG answer generation might have failed or produced mismatched results.\")\n",
    "else:\n",
    "    logger.warning(\"Vector index, Generator LLM, or test set not ready. Skipping RAG generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bd213",
   "metadata": {},
   "source": [
    "## 9. Analysis of Results & Conclusion\n",
    "\n",
    "Finally, we compare the evaluation metrics for the No-RAG (baseline) and RAG approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce1494b",
   "metadata": {},
   "source": [
    "### 9.1. Retrieval Evaluation (Recall@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9da090ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:13:18,699 - __main__ - INFO - Evaluating retrieval recall for K values: [1, 3, 5, 10]\n",
      "2025-06-01 15:13:18,700 - src.evaluation_utils - INFO - Evaluating recall with retriever top_k=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615baf9573534751b0a59c029f8a16bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Recall@K:   0%|          | 0/300 [00:00<?, ? questions/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieval Recall@K Scores ---\n",
      "Recall@1: 0.2900\n",
      "Recall@3: 0.5567\n",
      "Recall@5: 0.6867\n",
      "Recall@10: 0.8067\n"
     ]
    }
   ],
   "source": [
    "recall_at_k_scores = {}\n",
    "if vector_index and test_set and gold_map_for_recall:\n",
    "    logger.info(f\"Evaluating retrieval recall for K values: {cfg.TOP_K_FOR_RECALL_EVALUATION}\")\n",
    "    recall_at_k_scores = evaluate_recall_at_k(\n",
    "        test_set, \n",
    "        vector_index, \n",
    "        gold_map_for_recall, \n",
    "        cfg.TOP_K_FOR_RECALL_EVALUATION\n",
    "    )\n",
    "    print(\"\\n--- Retrieval Recall@K Scores ---\")\n",
    "    for k_val, score in recall_at_k_scores.items():\n",
    "        print(f\"Recall@{k_val}: {score:.4f}\")\n",
    "else:\n",
    "    logger.warning(\"Skipping Recall@K evaluation due to missing components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee184f",
   "metadata": {},
   "source": [
    "### 9.2. Generation Evaluation (ROUGE-L, BLEU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50018227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:13:22,763 - __main__ - INFO - Evaluating No-RAG generation (ROUGE/BLEU)...\n",
      "2025-06-01 15:13:22,764 - absl - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4cf992848d497891433e82061efd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating ROUGE & BLEU:   0%|          | 0/300 [00:00<?, ? pairs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- No-RAG Generation Metrics (ROUGE/BLEU) ---\n",
      "rougeL_fmeasure: 0.0703\n",
      "bleu_4: 0.0089\n",
      "2025-06-01 15:13:25,135 - __main__ - INFO - Evaluating RAG generation (ROUGE/BLEU)...\n",
      "2025-06-01 15:13:25,136 - absl - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5f122dffef4e429b7aacb3c83e1e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating ROUGE & BLEU:   0%|          | 0/300 [00:00<?, ? pairs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG Generation Metrics (ROUGE/BLEU) ---\n",
      "rougeL_fmeasure: 0.3318\n",
      "bleu_4: 0.1420\n"
     ]
    }
   ],
   "source": [
    "reference_answers = [item['target_answer'] for item in test_set] if test_set else []\n",
    "generation_metrics_no_rag = {}\n",
    "generation_metrics_rag = {}\n",
    "\n",
    "if no_rag_answers and reference_answers and len(no_rag_answers) == len(reference_answers):\n",
    "    logger.info(\"Evaluating No-RAG generation (ROUGE/BLEU)...\")\n",
    "    generation_metrics_no_rag = calculate_rouge_l_and_bleu(no_rag_answers, reference_answers)\n",
    "    print(\"\\n--- No-RAG Generation Metrics (ROUGE/BLEU) ---\")\n",
    "    for metric, score in generation_metrics_no_rag.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "else:\n",
    "    logger.warning(\"Skipping No-RAG ROUGE/BLEU evaluation (answers or references missing/mismatched).\")\n",
    "\n",
    "if rag_answers and reference_answers and len(rag_answers) == len(reference_answers):\n",
    "    logger.info(\"Evaluating RAG generation (ROUGE/BLEU)...\")\n",
    "    generation_metrics_rag = calculate_rouge_l_and_bleu(rag_answers, reference_answers)\n",
    "    print(\"\\n--- RAG Generation Metrics (ROUGE/BLEU) ---\")\n",
    "    for metric, score in generation_metrics_rag.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "else:\n",
    "    logger.warning(\"Skipping RAG ROUGE/BLEU evaluation (answers or references missing/mismatched).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a034d",
   "metadata": {},
   "source": [
    "### 9.3. Generation Evaluation (LLM-as-a-Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ee6a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:13:25,683 - __main__ - INFO - Evaluating No-RAG answers for Correctness/Relevance with LLM Judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ea1ca4b05e4130bb4d6a3e8832ee33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judge No-RAG Correct/Relevant:   0%|          | 0/300 [00:00<?, ? items/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:14:17,354 - src.evaluation_utils - WARNING - Judge prompt for query 'list of backward compatible games for the xbox one...' is too long: 14923 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:15:45,844 - src.evaluation_utils - WARNING - Judge prompt for query 'what are the 5 tallest mountains in the united sta...' is too long: 19392 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:17:58,944 - src.evaluation_utils - WARNING - Judge prompt for query 'players to win champions league with 3 teams...' is too long: 11397 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:18:58,727 - src.evaluation_utils - WARNING - Judge prompt for query 'what games can you play on xbox 360...' is too long: 15665 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:21:20,206 - src.evaluation_utils - WARNING - Judge prompt for query 'so xbox 360 games work on xbox one...' is too long: 16076 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "\n",
      "No-RAG LLM Judge Avg Correctness/Relevance: 0.4633\n",
      "2025-06-01 15:21:32,636 - __main__ - INFO - Evaluating RAG answers for Correctness/Relevance with LLM Judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afba93943304d95820245c4aa86b848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judge RAG Correct/Relevant:   0%|          | 0/300 [00:00<?, ? items/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:21:54,503 - src.evaluation_utils - WARNING - Judge prompt for query 'list of backward compatible games for the xbox one...' is too long: 14145 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:22:41,814 - src.evaluation_utils - WARNING - Judge prompt for query 'what are the 5 tallest mountains in the united sta...' is too long: 19251 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:23:29,418 - src.evaluation_utils - WARNING - Judge prompt for query 'players to win champions league with 3 teams...' is too long: 11260 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:23:57,579 - src.evaluation_utils - WARNING - Judge prompt for query 'what games can you play on xbox 360...' is too long: 14788 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "2025-06-01 15:24:45,564 - src.evaluation_utils - WARNING - Judge prompt for query 'so xbox 360 games work on xbox one...' is too long: 15738 tokens (JUDGE_MODEL_CTX_WINDOW 4096). Skipping.\n",
      "RAG LLM Judge Avg Correctness/Relevance: 0.7033\n",
      "2025-06-01 15:24:52,668 - __main__ - INFO - Evaluating RAG answers for Faithfulness to Context with LLM Judge...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca09e4d8dc0494eb0a337ec47d10d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judge RAG Faithfulness:   0%|          | 0/300 [00:00<?, ? items/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG LLM Judge Avg Faithfulness to Context: 0.9233\n"
     ]
    }
   ],
   "source": [
    "judge_correctness_relevance_prompt_str = (\n",
    "    \"You are an impartial AI assistant evaluating the correctness and relevance of a Generated Answer to a Question, \"\n",
    "    \"using a Reference Answer as a guide. Consider if the Generated Answer accurately addresses the Question and aligns \"\n",
    "    \"with the information expected, as exemplified by the Reference Answer. Do not be overly-strict on phrasing; \"\n",
    "    \"focus on semantic correctness and relevance.\\n\\n\"\n",
    "    \"Question: {query_str}\\n\"\n",
    "    \"Reference Answer: {reference_answer_str}\\n\"\n",
    "    \"Generated Answer: {generated_answer_str}\\n\\n\"\n",
    "    \"Is the Generated Answer correct and relevant for the given Question, when compared to the Reference Answer? \"\n",
    "    \"Respond with only YES or NO.\"\n",
    ")\n",
    "judge_correctness_relevance_prompt_tmpl = PromptTemplate(judge_correctness_relevance_prompt_str)\n",
    "\n",
    "judge_rag_faithfulness_prompt_str = (\n",
    "    \"You are an impartial AI assistant evaluating if a Generated Answer is faithful to the provided Retrieved Context. \"\n",
    "    \"The answer is faithful if all claims made in the answer are clearly supported by the Retrieved Context. \"\n",
    "    \"Do not use any external knowledge.\\n\\n\"\n",
    "    \"Retrieved Context: {context_str}\\n\"\n",
    "    \"Generated Answer: {generated_answer_str}\\n\\n\"\n",
    "    \"Is the Generated Answer faithful to the Retrieved Context? \"\n",
    "    \"Respond with only YES or NO.\"\n",
    ")\n",
    "judge_rag_faithfulness_prompt_tmpl = PromptTemplate(judge_rag_faithfulness_prompt_str)\n",
    "\n",
    "avg_correctness_relevance_no_rag = 0.0\n",
    "correctness_relevance_scores_no_rag = []\n",
    "if judge_llm and no_rag_answers and reference_answers and test_set and len(no_rag_answers) == len(test_set):\n",
    "    logger.info(\"Evaluating No-RAG answers for Correctness/Relevance with LLM Judge...\")\n",
    "    correctness_relevance_scores_no_rag = evaluate_with_llm_judge(\n",
    "        judge_llm, judge_correctness_relevance_prompt_tmpl, \n",
    "        test_set, no_rag_answers, references=reference_answers,\n",
    "        progress_desc=\"Judge No-RAG Correct/Relevant\"\n",
    "    )\n",
    "    if correctness_relevance_scores_no_rag:\n",
    "        avg_correctness_relevance_no_rag = np.mean(correctness_relevance_scores_no_rag)\n",
    "    print(f\"\\nNo-RAG LLM Judge Avg Correctness/Relevance: {avg_correctness_relevance_no_rag:.4f}\")\n",
    "else:\n",
    "    logger.warning(\"Skipping No-RAG LLM Judge (Correctness/Relevance) due to missing components or mismatched lengths.\")\n",
    "\n",
    "avg_correctness_relevance_rag = 0.0\n",
    "correctness_relevance_scores_rag = []\n",
    "if judge_llm and rag_answers and reference_answers and test_set and len(rag_answers) == len(test_set):\n",
    "    logger.info(\"Evaluating RAG answers for Correctness/Relevance with LLM Judge...\")\n",
    "    correctness_relevance_scores_rag = evaluate_with_llm_judge(\n",
    "        judge_llm, judge_correctness_relevance_prompt_tmpl, \n",
    "        test_set, rag_answers, references=reference_answers,\n",
    "        progress_desc=\"Judge RAG Correct/Relevant\"\n",
    "    )\n",
    "    if correctness_relevance_scores_rag:\n",
    "        avg_correctness_relevance_rag = np.mean(correctness_relevance_scores_rag)\n",
    "    print(f\"RAG LLM Judge Avg Correctness/Relevance: {avg_correctness_relevance_rag:.4f}\")\n",
    "else:\n",
    "    logger.warning(\"Skipping RAG LLM Judge (Correctness/Relevance) due to missing components or mismatched lengths.\")\n",
    "\n",
    "avg_faithfulness_rag = 0.0\n",
    "faithfulness_scores_rag = []\n",
    "if judge_llm and rag_answers and retrieved_contexts_for_rag and test_set and len(rag_answers) == len(test_set):\n",
    "    logger.info(\"Evaluating RAG answers for Faithfulness to Context with LLM Judge...\")\n",
    "    faithfulness_scores_rag = evaluate_with_llm_judge(\n",
    "        judge_llm, judge_rag_faithfulness_prompt_tmpl, \n",
    "        test_set, rag_answers, contexts=retrieved_contexts_for_rag,\n",
    "        progress_desc=\"Judge RAG Faithfulness\"\n",
    "    )\n",
    "    if faithfulness_scores_rag:\n",
    "        avg_faithfulness_rag = np.mean(faithfulness_scores_rag)\n",
    "    print(f\"RAG LLM Judge Avg Faithfulness to Context: {avg_faithfulness_rag:.4f}\")\n",
    "else:\n",
    "    logger.warning(\"Skipping RAG LLM Judge (Faithfulness) due to missing components or mismatched lengths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5bf26",
   "metadata": {},
   "source": [
    "## 10. Results Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8f4c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Overall Evaluation Metrics Summary ---\n",
      "Retrieval Recall@1: 0.2900\n",
      "Retrieval Recall@3: 0.5567\n",
      "Retrieval Recall@5: 0.6867\n",
      "Retrieval Recall@10: 0.8067\n",
      "\n",
      "--- No-RAG Generation Metrics ---\n",
      "No-RAG rougeL_fmeasure: 0.0703\n",
      "No-RAG bleu_4: 0.0089\n",
      "No-RAG LLM Judge Avg Correctness/Relevance: 0.4633\n",
      "\n",
      "--- RAG Generation Metrics ---\n",
      "RAG rougeL_fmeasure: 0.3318\n",
      "RAG bleu_4: 0.1420\n",
      "RAG LLM Judge Avg Correctness/Relevance: 0.7033\n",
      "RAG LLM Judge Avg Faithfulness to Context: 0.9233\n",
      "\n",
      "--- Sample Results DataFrame (First 5) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Reference Answer</th>\n",
       "      <th>No-RAG Answer</th>\n",
       "      <th>RAG Answer</th>\n",
       "      <th>Retrieved Context (RAG)</th>\n",
       "      <th>Judge Correct/Relevant (No-RAG)</th>\n",
       "      <th>Judge Correct/Relevant (RAG)</th>\n",
       "      <th>Judge Faithful to Context (RAG)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when does the movie shot caller come out in th...</td>\n",
       "      <td>August 18 , 2017</td>\n",
       "      <td>â€œShot Callerâ€ was released in theaters on **Au...</td>\n",
       "      <td>August 18, 2017</td>\n",
       "      <td>It premiered at the Los Angeles Film Festival ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>radius of curvature in case of plane mirror</td>\n",
       "      <td>Mathematically , a plane mirror can be conside...</td>\n",
       "      <td>Okay, let's break down the concept of radius o...</td>\n",
       "      <td>A plane mirror can be considered the limit of ...</td>\n",
       "      <td>A plane mirror is a mirror with a flat ( plana...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how many justices currently serve on the us su...</td>\n",
       "      <td>nine</td>\n",
       "      <td>As of today, November 2, 2023, there are **nin...</td>\n",
       "      <td>Nine</td>\n",
       "      <td>This article is part of the series on the Unit...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who is hosting the next world cup 2022</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>Qatar is hosting the next World Cup in 2022.</td>\n",
       "      <td>Qatar</td>\n",
       "      <td>On 19 March 2015 , FIFA sources confirmed that...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cohesion tension theory of water transport in ...</td>\n",
       "      <td>YES</td>\n",
       "      <td>Okay, let's break down the Cohesion-Tension Th...</td>\n",
       "      <td>The cohesion-tension theory explains water mov...</td>\n",
       "      <td>Water is constantly lost through transpiration...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  when does the movie shot caller come out in th...   \n",
       "1        radius of curvature in case of plane mirror   \n",
       "2  how many justices currently serve on the us su...   \n",
       "3             who is hosting the next world cup 2022   \n",
       "4  cohesion tension theory of water transport in ...   \n",
       "\n",
       "                                    Reference Answer  \\\n",
       "0                                   August 18 , 2017   \n",
       "1  Mathematically , a plane mirror can be conside...   \n",
       "2                                               nine   \n",
       "3                                              Qatar   \n",
       "4                                                YES   \n",
       "\n",
       "                                       No-RAG Answer  \\\n",
       "0  â€œShot Callerâ€ was released in theaters on **Au...   \n",
       "1  Okay, let's break down the concept of radius o...   \n",
       "2  As of today, November 2, 2023, there are **nin...   \n",
       "3       Qatar is hosting the next World Cup in 2022.   \n",
       "4  Okay, let's break down the Cohesion-Tension Th...   \n",
       "\n",
       "                                          RAG Answer  \\\n",
       "0                                    August 18, 2017   \n",
       "1  A plane mirror can be considered the limit of ...   \n",
       "2                                               Nine   \n",
       "3                                              Qatar   \n",
       "4  The cohesion-tension theory explains water mov...   \n",
       "\n",
       "                             Retrieved Context (RAG)  \\\n",
       "0  It premiered at the Los Angeles Film Festival ...   \n",
       "1  A plane mirror is a mirror with a flat ( plana...   \n",
       "2  This article is part of the series on the Unit...   \n",
       "3  On 19 March 2015 , FIFA sources confirmed that...   \n",
       "4  Water is constantly lost through transpiration...   \n",
       "\n",
       "   Judge Correct/Relevant (No-RAG)  Judge Correct/Relevant (RAG)  \\\n",
       "0                              0.0                           1.0   \n",
       "1                              1.0                           1.0   \n",
       "2                              1.0                           1.0   \n",
       "3                              1.0                           1.0   \n",
       "4                              1.0                           1.0   \n",
       "\n",
       "   Judge Faithful to Context (RAG)  \n",
       "0                                1  \n",
       "1                                1  \n",
       "2                                1  \n",
       "3                                1  \n",
       "4                                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-01 15:33:43,257 - __main__ - INFO - Results DataFrame saved to /home/denis/kpi/iasa_nlp_labs/nq_rag/results/nq_rag_results.csv\n"
     ]
    }
   ],
   "source": [
    "results_data = []\n",
    "if test_set:\n",
    "    for i, item in enumerate(test_set):\n",
    "        res_item = {\n",
    "            'Question': item['question_text'],\n",
    "            'Reference Answer': item['target_answer'],\n",
    "            'No-RAG Answer': no_rag_answers[i] if i < len(no_rag_answers) else 'N/A',\n",
    "            'RAG Answer': rag_answers[i] if i < len(rag_answers) else 'N/A',\n",
    "            'Retrieved Context (RAG)': (retrieved_contexts_for_rag[i][:500] + \"...\" \n",
    "                                       if i < len(retrieved_contexts_for_rag) and \n",
    "                                          isinstance(retrieved_contexts_for_rag[i], str) and \n",
    "                                          retrieved_contexts_for_rag[i] not in [\"ERROR_RETRIEVING_CONTEXT\", None] \n",
    "                                       else (retrieved_contexts_for_rag[i] if i < len(retrieved_contexts_for_rag) else 'N/A')),\n",
    "            'Judge Correct/Relevant (No-RAG)': correctness_relevance_scores_no_rag[i] if i < len(correctness_relevance_scores_no_rag) else 'N/A',\n",
    "            'Judge Correct/Relevant (RAG)': correctness_relevance_scores_rag[i] if i < len(correctness_relevance_scores_rag) else 'N/A',\n",
    "            'Judge Faithful to Context (RAG)': faithfulness_scores_rag[i] if i < len(faithfulness_scores_rag) else 'N/A'\n",
    "        }\n",
    "        results_data.append(res_item)\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n--- Overall Evaluation Metrics Summary ---\")\n",
    "if recall_at_k_scores:\n",
    "    for k, score in recall_at_k_scores.items():\n",
    "        print(f\"Retrieval Recall@{k}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"Retrieval Recall: Not computed or no data.\")\n",
    "\n",
    "print(\"\\n--- No-RAG Generation Metrics ---\")\n",
    "if generation_metrics_no_rag:\n",
    "    for metric, score in generation_metrics_no_rag.items():\n",
    "        print(f\"No-RAG {metric}: {score:.4f}\")\n",
    "else: print(\"No-RAG ROUGE/BLEU: Not computed or no data.\")\n",
    "print(f\"No-RAG LLM Judge Avg Correctness/Relevance: {avg_correctness_relevance_no_rag:.4f}\")\n",
    "\n",
    "print(\"\\n--- RAG Generation Metrics ---\")\n",
    "if generation_metrics_rag:\n",
    "    for metric, score in generation_metrics_rag.items():\n",
    "        print(f\"RAG {metric}: {score:.4f}\")\n",
    "else: print(\"RAG ROUGE/BLEU: Not computed or no data.\")\n",
    "print(f\"RAG LLM Judge Avg Correctness/Relevance: {avg_correctness_relevance_rag:.4f}\")\n",
    "print(f\"RAG LLM Judge Avg Faithfulness to Context: {avg_faithfulness_rag:.4f}\")\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"\\n--- Sample Results DataFrame (First 5) ---\")\n",
    "    display(results_df.head())\n",
    "    # To save to CSV:\n",
    "    results_df.to_csv(os.path.join(cfg.PROJECT_ROOT, \"results\", \"nq_rag_results.csv\"), index=False)\n",
    "    logger.info(f\"Results DataFrame saved to {os.path.join(cfg.PROJECT_ROOT, 'results', 'nq_rag_results.csv')}\")\n",
    "else:\n",
    "    print(\"\\nNo results to display in the DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
